import requests
import os
import sys
import argparse
import io
import multiprocessing
import time
from queue import Empty
from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor

def get_auth_headers():
    token = os.environ.get("HF_TOKEN")
    if not token:
        print("[ERROR] ç¯å¢ƒå˜é‡ HF_TOKEN æœªè®¾ç½®ï¼Œæ— æ³•è®¤è¯ Artifactoryã€‚", file=sys.stderr)
        sys.exit(1)
    return {"Authorization": f"Bearer {token}"}

def get_files_from_artifactory_api(repo_id, revision, artifactory_base, timeout=20, debug=False, repo_type="model"):
    """
    è·å– Artifactory ä»£ç†ä»“åº“ä¸‹æŒ‡å®šæ¨¡å‹æˆ–æ•°æ®é›†çš„æ‰€æœ‰æ–‡ä»¶åˆ—è¡¨
    repo_type: "model" æˆ– "dataset"
    """
    if repo_type == "dataset":
        api_url = f"{artifactory_base}/api/datasets/{repo_id}/revision/{revision}"
    else:
        api_url = f"{artifactory_base}/api/models/{repo_id}/revision/{revision}"
    headers = get_auth_headers()
    if debug:
        print(f"[DEBUG] è·å–æ–‡ä»¶åˆ—è¡¨: {api_url}")
    resp = requests.get(api_url, headers=headers, timeout=timeout)
    if resp.status_code != 200:
        raise Exception(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {resp.status_code} {resp.text}")
    data = resp.json()
    return data.get("siblings", [])

def print_model_result(repo_type, repo_id, revision, model_output_lines, completed_count=None, total_count=None):
    print(f"\nğŸ” {repo_type}: {repo_id}@{revision}")
    print("-" * 60)
    cached, not_cached, error = 0, 0, 0
    for line in model_output_lines:
        print(line, end="")
        if "\033[32mCACHED\033[0m" in line:
            cached += 1
        elif "\033[35mNOT CACHED\033[0m" in line:
            not_cached += 1
        elif "\033[31mERROR" in line:
            error += 1
    print(f"\nç»Ÿè®¡: \033[32mCACHED {cached}\033[0m, \033[35mNOT CACHED {not_cached}\033[0m, \033[31mERROR {error}\033[0m")
    if completed_count is not None and total_count is not None:
        print(f"âœ… {repo_type} {repo_id}@{revision} å¤„ç†å®Œæˆ ({completed_count}/{total_count})")
    print("------------------------------------------------------------")

def file_head_check_and_queue(repo_id, revision, artifactory_base, f, headers, file_timeout, queue, repo_type="model"):
    filename = f['rfilename'] if isinstance(f, dict) and 'rfilename' in f else str(f)
    if repo_type == "dataset":
        url = f"{artifactory_base}/datasets/{repo_id}/resolve/{revision}/{filename}"
    else:
        url = f"{artifactory_base}/{repo_id}/resolve/{revision}/{filename}"
    try:
        resp = requests.head(url, headers=headers, timeout=file_timeout)
        if resp.status_code == 200:
            status = "CACHED"
        elif resp.status_code == 404:
            status = "NOT CACHED"
        else:
            status = f"ERROR {resp.status_code}"
    except requests.exceptions.Timeout:
        status = "NOT CACHED"
    except Exception as e:
        status = f"ERROR {e}"
    if status == "NOT CACHED":
        colored_status = "\033[35mNOT CACHED\033[0m"  # ç´«è‰²
    elif status == "CACHED":
        colored_status = "\033[32mCACHED\033[0m"      # ç»¿è‰²
    else:
        colored_status = f"\033[31m{status}\033[0m"   # çº¢è‰²ï¼ˆé”™è¯¯ï¼‰
    output_line = f"{filename:<40} | {colored_status:>10}\n"
    queue.put(output_line)

def cache_from_remote_worker(repo_id, revision, artifactory_base, timeout, debug, file_workers, file_timeout, queue, repo_type="model"):
    import threading
    files = get_files_from_artifactory_api(repo_id, revision, artifactory_base, timeout, debug, repo_type)
    queue.put(f"[{repo_id}@{revision}] å…±{len(files)}ä¸ªæ–‡ä»¶ï¼Œå°†æ£€æµ‹ Artifactory ä»£ç†ç¼“å­˜ï¼š\n")
    queue.put(f"__FILE_COUNT__:{len(files)}")
    headers = get_auth_headers()
    threads = []
    for f in files:
        t = threading.Thread(target=file_head_check_and_queue, args=(repo_id, revision, artifactory_base, f, headers, file_timeout, queue, repo_type))
        t.start()
        threads.append(t)
    for t in threads:
        t.join()
    queue.put("__MODEL_DONE__")

def cache_from_remote_with_timeout(repo_id, revision, artifactory_base, timeout=20, debug=False, file_workers=12, file_timeout=5, silent=False, repo_type="model"):
    from multiprocessing import Process, Queue
    from queue import Empty
    import time
    output = ""
    queue = Queue()
    p = Process(target=cache_from_remote_worker, args=(repo_id, revision, artifactory_base, timeout, debug, file_workers, file_timeout, queue, repo_type))
    p.start()
    start_time = time.time()
    model_done = False
    file_count = None
    files_received = 0
    while not model_done and (time.time() - start_time) <= timeout:
        try:
            msg = queue.get(timeout=0.2)
            if msg == "__MODEL_DONE__":
                model_done = True
            elif msg.startswith("__FILE_COUNT__:"):
                file_count = int(msg.split(":", 1)[1])
            else:
                if not silent:
                    print(msg, end="")
                output += msg
                if " | " in msg and not msg.startswith("[") and not msg.startswith("[ERROR]"):
                    files_received += 1
                    if file_count is not None and files_received >= file_count:
                        model_done = True
                        p.terminate()
                        break
        except Empty:
            if not p.is_alive():
                break
            pass
    if model_done and file_count is not None and files_received >= file_count:
        p.terminate()
        p.kill()
        return output
    while True:
        try:
            msg = queue.get_nowait()
            if msg != "__MODEL_DONE__" and not msg.startswith("__FILE_COUNT__:"):
                if not silent:
                    print(msg, end="")
                output += msg
        except Empty:
            break
    if not model_done:
        p.terminate()
        p.join()
        timeout_msg = f"[TIMEOUT] {repo_id}@{revision} æ£€æµ‹è¶…æ—¶ï¼ˆ>{timeout}sï¼‰\n"
        if not silent:
            print(timeout_msg, end="")
        output += timeout_msg
    else:
        p.join()
    return output

def parse_checklist_file(filename):
    tasks = []
    with open(filename, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            parts = [x.strip() for x in line.split(",")]
            if len(parts) == 3:
                repo_type, repo_id, revision = parts
                if repo_type not in ("model", "dataset"):
                    print(f"[WARN] æœªçŸ¥ç±»å‹: {repo_type}, é»˜è®¤æŒ‰ model å¤„ç†")
                    repo_type = "model"
                tasks.append((repo_type, repo_id, revision))
            elif len(parts) == 2:
                # å…¼å®¹è€æ ¼å¼
                repo_id, revision = parts
                tasks.append(("model", repo_id, revision))
            else:
                print(f"[WARN] æ ¼å¼ä¸æ­£ç¡®: {line}")
    return tasks

def batch_from_file_concurrent(models_file, artifactory_base, timeout=20, debug=False, workers=2, file_workers=12, file_timeout=5):
    tasks = parse_checklist_file(models_file)
    print(f"å¼€å§‹å¹¶å‘å¤„ç† {len(tasks)} ä¸ªæ¡ç›®ï¼Œå¹¶å‘æ•°: {workers}")
    from functools import partial
    from concurrent.futures import as_completed, ProcessPoolExecutor
    with ProcessPoolExecutor(max_workers=workers) as executor:
        future_to_task = {
            executor.submit(
                cache_from_remote_with_timeout,
                repo_id, revision, artifactory_base, timeout, debug, file_workers, file_timeout, True, repo_type
            ): (repo_type, repo_id, revision)
            for repo_type, repo_id, revision in tasks
        }
        completed_count = 0
        total_count = len(tasks)
        for future in as_completed(future_to_task):
            repo_type, repo_id, revision = future_to_task[future]
            try:
                result = future.result(timeout=timeout + 5)
                completed_count += 1
                model_output_lines = result.splitlines(keepends=True)
                print_model_result(repo_type, repo_id, revision, model_output_lines, completed_count, total_count)
            except Exception as e:
                completed_count += 1
                print(f"\nğŸ” {repo_type}: {repo_id}@{revision}")
                print("-" * 60)
                print(f"âŒ {repo_type} {repo_id}@{revision} å¤„ç†å¤±è´¥: {e}\n")
                print("------")
                print(f"âŒ {repo_type} {repo_id}@{revision} å¤„ç†å¤±è´¥ ({completed_count}/{total_count})")
        print(f"\nğŸ‰ æ‰€æœ‰ {len(tasks)} ä¸ªæ¡ç›®å¤„ç†å®Œæˆï¼")

def batch_from_file(models_file, artifactory_base, timeout=20, debug=False, workers=2, file_workers=12, file_timeout=5):
    tasks = parse_checklist_file(models_file)
    total_count = len(tasks)
    for idx, (repo_type, repo_id, revision) in enumerate(tasks, 1):
        print(f"\nå¼€å§‹å¤„ç†{repo_type}: {repo_id}@{revision}")
        result = cache_from_remote_with_timeout(repo_id, revision, artifactory_base, timeout, debug, file_workers, file_timeout, repo_type=repo_type)
        model_output_lines = result.splitlines(keepends=True)
        print_model_result(repo_type, repo_id, revision, model_output_lines, idx, total_count)

if __name__ == "__main__":
    import sys
    from datetime import datetime
    
    # é‡å®šå‘è¾“å‡ºåˆ°æ–‡ä»¶
    class Logger:
        def __init__(self, filename):
            self.terminal = sys.stdout
            self.log = open(filename, 'w', encoding='utf-8')
            
        def write(self, message):
            self.terminal.write(message)
            self.log.write(message)
            self.log.flush()
            
        def flush(self):
            self.terminal.flush()
            self.log.flush()
    
    # è®¾ç½®æ—¥å¿—æ–‡ä»¶
    log_filename = "head.log"
    sys.stdout = Logger(log_filename)
    
    # å†™å…¥æ—¥å¿—å¤´éƒ¨ä¿¡æ¯
    print(f"# Artifactory ä»£ç†ç¼“å­˜æ£€æµ‹æ—¥å¿—")
    print(f"# å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"# å‘½ä»¤: {' '.join(sys.argv)}")
    print("=" * 80)
    
    parser = argparse.ArgumentParser(description="æ£€æµ‹ Artifactory ä»£ç†ç¼“å­˜çŠ¶æ€ï¼Œæ”¯æŒæ‰¹é‡å’Œå¹¶å‘ï¼Œä¸¥æ ¼è¶…æ—¶ï¼Œæ¨¡å‹å†…æ–‡ä»¶å¤šçº¿ç¨‹ï¼Œå®æ—¶è¾“å‡º")
    parser.add_argument("--models", type=str, default=None, help="æ¨¡å‹åˆ—è¡¨æ–‡ä»¶ï¼Œæ¯è¡Œ: repo_id,revision")
    parser.add_argument("--timeout", type=int, default=15, help="æ¯ä¸ªæ¨¡å‹æ£€æµ‹çš„æœ€å¤§è¶…æ—¶ï¼ˆç§’ï¼‰")
    parser.add_argument("--debug", action="store_true", help="æ‰“å° API åŸå§‹å†…å®¹")
    parser.add_argument("--registry", type=str, default="http://localhost:8082/artifactory/api/huggingfaceml/huggingfaceml-remote", help="Artifactory ä»£ç†åŸºç¡€URL")
    parser.add_argument("--workers", type=int, default=1, help="å¹¶å‘è¿›ç¨‹æ•°ï¼ˆæ¨¡å‹çº§å¹¶å‘ï¼‰ï¼Œworkers<=1ä¸ºå•çº¿ç¨‹ï¼Œworkers>1ä¸ºå¹¶å‘æ¨¡å¼")
    parser.add_argument("--file-workers", type=int, default=10, help="æ¯ä¸ªæ¨¡å‹å†…éƒ¨æ–‡ä»¶å¹¶å‘æ•°")
    parser.add_argument("--file-timeout", type=int, default=5, help="æ¯ä¸ªæ–‡ä»¶ HEAD æ£€æŸ¥è¶…æ—¶ï¼ˆç§’ï¼‰")
    args = parser.parse_args()

    try:
        if args.models:
            if args.workers > 1:
                print(f"ğŸš€ ä½¿ç”¨å¹¶å‘ç‰ˆæœ¬å¤„ç†ï¼Œå¹¶å‘æ•°: {args.workers}")
                batch_from_file_concurrent(args.models, args.registry, timeout=args.timeout, debug=args.debug, workers=args.workers, file_workers=args.file_workers, file_timeout=args.file_timeout)
            else:
                print("ğŸŒ ä½¿ç”¨å•çº¿ç¨‹ç‰ˆæœ¬å¤„ç†")
                batch_from_file(args.models, args.registry, timeout=args.timeout, debug=args.debug, workers=args.workers, file_workers=args.file_workers, file_timeout=args.file_timeout)
        else:
            # å•æ¨¡å‹æµ‹è¯•ï¼ˆ1.2Gï¼‰
            repo_id = "LiheYoung/depth_anything_vitl14"
            revision = "973948530e4e4f4afd6d1913f670d9f96071dcaa"
            cache_from_remote_with_timeout(repo_id, revision, args.registry, timeout=args.timeout, debug=args.debug, file_workers=args.file_workers, file_timeout=args.file_timeout)
    finally:
        # å†™å…¥æ—¥å¿—å°¾éƒ¨ä¿¡æ¯
        print("=" * 80)
        print(f"# ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"# æ—¥å¿—å·²ä¿å­˜åˆ°: {log_filename}")
        
        # æ¢å¤æ ‡å‡†è¾“å‡º
        sys.stdout.log.close()
        sys.stdout = sys.stdout.terminal