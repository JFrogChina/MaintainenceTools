import hashlib
import os
from pathlib import Path
import pandas as pd
import argparse
import re
from collections import defaultdict
import random

# -----------------------------
# ğŸ”§ CLI å‚æ•°è§£æ
# -----------------------------
def parse_chunk_size(s):
    s = s.strip().upper()
    match = re.match(r"(\d+)(B|KB|MB|GB)?", s)
    if not match:
        raise argparse.ArgumentTypeError("Invalid chunk size format")
    num, unit = match.groups()
    multiplier = {
        None: 1,
        "B": 1,
        "KB": 1024,
        "MB": 1024 ** 2,
        "GB": 1024 ** 3
    }[unit]
    size = int(num) * multiplier
    if size < 1:
        raise argparse.ArgumentTypeError("Chunk size must be >= 1 byte")
    return size

parser = argparse.ArgumentParser(description="Deduplication check for chunked files")
parser.add_argument("--chunk-size", default="10MB", type=parse_chunk_size,
                    help="Chunk size, e.g., 512KB, 1MB, 2048 (bytes)")
parser.add_argument("--dir", default="logs", help="Directory to scan (default: ./logs)")
args = parser.parse_args()

CHUNK_SIZE = args.chunk_size
LOGS_DIR = args.dir

print(f"ğŸ”§ Chunk size: {CHUNK_SIZE} bytes")
print(f"ğŸ“‚ Scanning directory: {LOGS_DIR}")

# -----------------------------
# ğŸ“ éå†æ–‡ä»¶
# -----------------------------
log_files = sorted(Path(LOGS_DIR).rglob("*"))
log_files = [f for f in log_files if f.is_file()]

if not log_files:
    print(f"âŒ No files found in '{LOGS_DIR}'")
    exit(1)

print(f"ğŸ“ Found {len(log_files)} files.")

# -----------------------------
# ğŸ§® åˆ†ç‰‡å¹¶è®¡ç®— SHA1
# -----------------------------
all_hashes = []
file_hash_info = []

def split_and_hash(file_path):
    hashes = []
    with open(file_path, "rb") as f:
        while chunk := f.read(CHUNK_SIZE):
            sha1 = hashlib.sha1(chunk).hexdigest()
            hashes.append(sha1)
    return hashes

for log_file in log_files:
    hashes = split_and_hash(log_file)
    all_hashes.extend(hashes)
    file_hash_info.append({
        "file": str(log_file),
        "chunks": len(hashes)
    })

# -----------------------------
# ğŸ“Š å»é‡ç‡ç»Ÿè®¡
# -----------------------------
total_chunks = len(all_hashes)
unique_chunks = len(set(all_hashes))
duplicate_chunks = total_chunks - unique_chunks
dedup_ratio = round((duplicate_chunks / total_chunks) * 100, 2) if total_chunks > 0 else 0.0

# -----------------------------
# ğŸ“„ æ¯æ–‡ä»¶ chunk æ•°é‡
# -----------------------------
print("\nğŸ“„ Per-File Chunk Count:")
df_files = pd.DataFrame(file_hash_info)
print(df_files.to_string(index=False))

# -----------------------------
# ğŸ“Š æ€»ä½“ç»Ÿè®¡
# -----------------------------
print("\nğŸ“Š === Overall Deduplication Summary ===")
print(f"Total Chunks          : {total_chunks}")
print(f"Unique Chunks         : {unique_chunks}")
print(f"Duplicate Chunks      : {duplicate_chunks}")
print(f"Deduplication Ratio   : {dedup_ratio}%")

# -----------------------------
# ğŸ” ä»»æ„ 10 ä¸ªé‡å¤ chunk å†…å®¹ï¼ˆæ‰“å°å‰ 1000 å­—èŠ‚ï¼‰
# -----------------------------
print("\nğŸ” Analyzing up to 10 random duplicate chunks (first 1000 bytes)...")

chunk_index = defaultdict(list)

# å»ºç«‹é‡å¤ chunk ç´¢å¼•ï¼Œå¹¶è®°å½•å®Œæ•´ chunk æ•°æ®
for log_file in log_files:
    with open(log_file, "rb") as f:
        offset = 0
        while chunk := f.read(CHUNK_SIZE):
            sha1 = hashlib.sha1(chunk).hexdigest()
            if all_hashes.count(sha1) > 1:
                chunk_index[sha1].append({
                    "file": str(log_file),
                    "offset": offset,
                    "content": chunk
                })
            offset += CHUNK_SIZE

# éšæœºæŠ½å–æœ€å¤š 10 ä¸ªé‡å¤ chunk
duplicate_sha1_list = list(chunk_index.items())
random.shuffle(duplicate_sha1_list)
sampled_duplicates = duplicate_sha1_list[:10]

if not sampled_duplicates:
    print("âŒ No duplicated chunks found.")
else:
    for i, (sha1, entries) in enumerate(sampled_duplicates, 1):
        print(f"\n#{i}: SHA1 = {sha1}  ({len(entries)} occurrences)")
        for entry in entries:
            print(f"\n--- Chunk from File: {entry['file']} | Offset: {entry['offset']} bytes ---")
            # åªæ‰“å°å‰ 1000 å­—èŠ‚
            content = entry['content'][:1000]
            try:
                text = content.decode('utf-8')
            except UnicodeDecodeError:
                text = content.decode('utf-8', errors='replace')
            print(text)
            print("--- End of Chunk Preview ---")
