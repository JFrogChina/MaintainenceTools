#!/usr/bin/env python3
"""
Binary SHA1 Validator
JFrog Artifactory åˆ¶å“SHA1æ ¡éªŒå·¥å…·

åŠŸèƒ½ï¼šæ£€æŸ¥Artifactoryåº•å±‚å­˜å‚¨ä¸­æ–‡ä»¶åå’ŒSHA1å€¼æ˜¯å¦ä¸€è‡´
ä¼˜åŠ¿ï¼šç›¸æ¯”shellè„šæœ¬ï¼ŒPythonå®ç°æ•ˆç‡æ›´é«˜ï¼Œé”™è¯¯å¤„ç†æ›´å®Œå–„
ä¼˜åŒ–ï¼šæ”¯æŒæµå¼å¤„ç†ã€åˆ†æ‰¹å¤„ç†å’Œæ–­ç‚¹ç»­ä¼ ï¼Œé€‚åˆå¤„ç†å¤§é‡æ–‡ä»¶
"""

import os
import sys
import json
import hashlib
import argparse
import threading
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple, Dict, Optional, Generator
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('checksum_validation.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class SHA1Validator:
    """SHA1æ ¡éªŒå™¨ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒå¤§é‡æ–‡ä»¶å¤„ç†"""
    
    def __init__(self, base_dir: str, thread_num: int = 4, batch_size: int = 10000):
        self.base_dir = Path(base_dir)
        self.thread_num = thread_num
        self.batch_size = batch_size
        self.results = {
            'valid': 0,
            'invalid': 0,
            'errors': 0,
            'total': 0
        }
        self.error_files = []
        self.processed_files = set()  # è®°å½•å·²å¤„ç†çš„æ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
        self.lock = threading.Lock()  # çº¿ç¨‹å®‰å…¨é”
        self.last_batch_count = 0  # è®°å½•ä¸Šæ¬¡çš„æ‰¹æ¬¡è®¡æ•°ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
        
    def calculate_sha1(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶çš„SHA1å€¼"""
        try:
            sha1_hash = hashlib.sha1()
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    sha1_hash.update(chunk)
            return sha1_hash.hexdigest()
        except Exception as e:
            logger.error(f"è®¡ç®—SHA1å¤±è´¥ {file_path}: {e}")
            return ""
    
    def is_sha1_filename(self, filename: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦ä¸ºSHA1æ ¼å¼ï¼ˆ40ä½åå…­è¿›åˆ¶ï¼‰"""
        return len(filename) == 40 and all(c in '0123456789abcdefABCDEF' for c in filename)
    
    def validate_file(self, file_path: Path) -> Tuple[bool, str]:
        """éªŒè¯å•ä¸ªæ–‡ä»¶çš„SHA1"""
        try:
            filename = file_path.name
            if not self.is_sha1_filename(filename):
                return False, f"æ–‡ä»¶åä¸æ˜¯SHA1æ ¼å¼: {filename}"
            
            calculated_sha1 = self.calculate_sha1(file_path)
            if not calculated_sha1:
                return False, f"æ— æ³•è®¡ç®—SHA1: {filename}"
            
            if calculated_sha1.lower() == filename.lower():
                return True, "SHA1åŒ¹é…"
            else:
                return False, f"SHA1ä¸åŒ¹é…: æœŸæœ›={filename}, å®é™…={calculated_sha1}"
                
        except Exception as e:
            return False, f"éªŒè¯å¤±è´¥: {e}"
    
    def find_artifact_files_generator(self, start_time: Optional[str] = None, end_time: Optional[str] = None) -> Generator[Path, None, None]:
        """ç”Ÿæˆå™¨æ–¹å¼æŸ¥æ‰¾æ–‡ä»¶ï¼Œæ”¯æŒæ—¶é—´è¿‡æ»¤ï¼Œé¿å…å†…å­˜å ç”¨"""
        try:
            for file_path in self.base_dir.rglob('*'):
                if file_path.is_file() and self.is_sha1_filename(file_path.name):
                    # å¦‚æœæŒ‡å®šäº†æ—¶é—´èŒƒå›´ï¼Œæ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                    if start_time and end_time:
                        try:
                            mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
                            start_dt = datetime.strptime(start_time, '%Y-%m-%d %H:%M')
                            end_dt = datetime.strptime(end_time, '%Y-%m-%d %H:%M')
                            if not (start_dt <= mtime <= end_dt):
                                continue
                        except Exception:
                            continue  # è·³è¿‡æ— æ³•è·å–æ—¶é—´çš„æ–‡ä»¶
                    
                    yield file_path
                        
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾æ–‡ä»¶å¤±è´¥: {e}")
    
    def process_batch(self, batch: List[Path]) -> Dict[str, int]:
        """å¤„ç†ä¸€æ‰¹æ–‡ä»¶"""
        batch_results = {'valid': 0, 'invalid': 0, 'errors': 0}
        
        with ThreadPoolExecutor(max_workers=self.thread_num) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {executor.submit(self.validate_file, file_path): file_path 
                            for file_path in batch}
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    is_valid, message = future.result()
                    
                    with self.lock:
                        self.results['total'] += 1
                        
                        if is_valid:
                            batch_results['valid'] += 1
                            self.results['valid'] += 1
                            logger.debug(f"âœ… {file_path.name}: {message}")
                        else:
                            batch_results['invalid'] += 1
                            self.results['invalid'] += 1
                            self.error_files.append((str(file_path), message))
                            logger.warning(f"âŒ {file_path.name}: {message}")
                        
                        # è®°å½•å·²å¤„ç†æ–‡ä»¶
                        self.processed_files.add(str(file_path))
                        
                except Exception as e:
                    batch_results['errors'] += 1
                    self.results['errors'] += 1
                    logger.error(f"éªŒè¯å¼‚å¸¸ {file_path}: {e}")
        
        return batch_results
    
    def validate_files_streaming(self, start_time: Optional[str] = None, end_time: Optional[str] = None) -> None:
        """æµå¼å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼Œæ”¯æŒåˆ†æ‰¹å¤„ç†å’Œæ–­ç‚¹ç»­ä¼ """
        logger.info(f"å¼€å§‹æµå¼éªŒè¯ï¼Œæ‰¹æ¬¡å¤§å°: {self.batch_size}, çº¿ç¨‹æ•°: {self.thread_num}")
        
        batch = []
        batch_count = self.last_batch_count  # ä»ä¸Šæ¬¡çš„æ‰¹æ¬¡è®¡æ•°ç»§ç»­
        
        for file_path in self.find_artifact_files_generator(start_time, end_time):
            # è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
            if str(file_path) in self.processed_files:
                continue
                
            batch.append(file_path)
            
            # è¾¾åˆ°æ‰¹æ¬¡å¤§å°æ—¶å¤„ç†
            if len(batch) >= self.batch_size:
                batch_count += 1
                logger.info(f"å¤„ç†ç¬¬ {batch_count} æ‰¹ï¼Œæ–‡ä»¶æ•°: {len(batch)}")
                
                batch_results = self.process_batch(batch)
                self._log_batch_progress(batch_count, batch_results)
                
                # ç«‹å³æ›´æ–°æ‰¹æ¬¡è®¡æ•°ï¼Œç¡®ä¿ä¸­æ–­æ—¶èƒ½æ­£ç¡®ä¿å­˜
                self.last_batch_count = batch_count
                
                batch = []  # æ¸…ç©ºæ‰¹æ¬¡
        
        # å¤„ç†æœ€åä¸€æ‰¹
        if batch:
            batch_count += 1
            logger.info(f"å¤„ç†æœ€åä¸€æ‰¹ï¼Œæ–‡ä»¶æ•°: {len(batch)}")
            batch_results = self.process_batch(batch)
            self._log_batch_progress(batch_count, batch_results)
            
            # æ›´æ–°æœ€åçš„æ‰¹æ¬¡è®¡æ•°
            self.last_batch_count = batch_count
        
        # å¦‚æœè¿˜æœ‰æœªå¤„ç†çš„æ–‡ä»¶ï¼Œæ›´æ–°ä¸ºä¸‹ä¸€ä¸ªæ‰¹æ¬¡
        if batch:
            self.last_batch_count = batch_count + 1
        
        logger.info("æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆ")
    
    def _log_batch_progress(self, batch_num: int, batch_results: Dict[str, int]) -> None:
        """è®°å½•æ‰¹æ¬¡è¿›åº¦"""
        # ç»Ÿä¸€æ ¼å¼ï¼šæ‰¹æ¬¡è¿›åº¦ + æ€»ä½“è¿›åº¦
        total_processed = self.results['total']
        
        # æ‰¹æ¬¡è¿›åº¦ï¼ˆç®€æ´ï¼‰
        logger.info(f"âœ… æ‰¹æ¬¡ {batch_num} | æ–‡ä»¶: {batch_results['valid'] + batch_results['invalid'] + batch_results['errors']} | "
                   f"é€šè¿‡: {batch_results['valid']} | å¤±è´¥: {batch_results['invalid']} | é”™è¯¯: {batch_results['errors']}")
        
        # æ€»ä½“è¿›åº¦ï¼ˆæ¯æ‰¹æ¬¡éƒ½æ˜¾ç¤ºï¼Œæ ¼å¼ç»Ÿä¸€ï¼‰
        logger.info(f"ğŸ“Š æ€»è®¡: {total_processed} æ–‡ä»¶ | "
                   f"é€šè¿‡: {self.results['valid']} | å¤±è´¥: {self.results['invalid']} | é”™è¯¯: {self.results['errors']}")
    
    def save_progress(self, filename: str = 'progress.json') -> None:
        """ä¿å­˜è¿›åº¦ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ """
        progress_data = {
            'processed_files': list(self.processed_files),
            'results': self.results,
            'error_files': self.error_files,
            'last_batch_count': self.last_batch_count,  # ä¿å­˜æ‰¹æ¬¡è®¡æ•°
            'timestamp': datetime.now().isoformat()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è¿›åº¦å·²ä¿å­˜åˆ° {filename}")
    
    def load_progress(self, filename: str = 'progress.json') -> bool:
        """åŠ è½½è¿›åº¦ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ """
        if os.path.exists(filename):
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    progress_data = json.load(f)
                
                self.processed_files = set(progress_data.get('processed_files', []))
                self.results = progress_data.get('results', self.results)
                self.error_files = progress_data.get('error_files', [])
                self.last_batch_count = progress_data.get('last_batch_count', 0) # åŠ è½½æ‰¹æ¬¡è®¡æ•°
                
                logger.info(f"å·²åŠ è½½è¿›åº¦: {len(self.processed_files)} ä¸ªæ–‡ä»¶å·²å¤„ç†")
                return True
            except Exception as e:
                logger.warning(f"åŠ è½½è¿›åº¦å¤±è´¥: {e}")
                return False
        return False
    
    def generate_report(self) -> str:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("Binary SHA1 éªŒè¯æŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"éªŒè¯æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"åŸºç¡€ç›®å½•: {self.base_dir}")
        report.append(f"æ€»æ–‡ä»¶æ•°: {self.results['total']}")
        report.append(f"éªŒè¯é€šè¿‡: {self.results['valid']}")
        report.append(f"éªŒè¯å¤±è´¥: {self.results['invalid']}")
        report.append(f"å¤„ç†é”™è¯¯: {self.results['errors']}")
        report.append("")
        
        if self.error_files:
            report.append("âŒ éªŒè¯å¤±è´¥çš„æ–‡ä»¶:")
            report.append("-" * 40)
            for file_path, error_msg in self.error_files:
                report.append(f"æ–‡ä»¶: {file_path}")
                report.append(f"é”™è¯¯: {error_msg}")
                report.append("")
        
        return "\n".join(report)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Binary SHA1 Validator - JFrog Artifactoryåˆ¶å“SHA1æ ¡éªŒå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # éªŒè¯æ‰€æœ‰æ–‡ä»¶
  python3 validator.py /opt/jfrog/artifactory/var/data/artifactory/filestore
  
  # æŒ‡å®šçº¿ç¨‹æ•°å’Œæ‰¹æ¬¡å¤§å°
  python3 validator.py /opt/jfrog/artifactory/var/data/artifactory/filestore --threads 8 --batch-size 20000
  
  # æŒ‡å®šæ—¶é—´èŒƒå›´
  python3 validator.py /opt/jfrog/artifactory/var/data/artifactory/filestore --start-time "2024-01-01 00:00" --end-time "2024-01-31 23:59"
  
  # ä»ä¸Šæ¬¡è¿›åº¦ç»§ç»­
  python3 validator.py /opt/jfrog/artifactory/var/data/artifactory/filestore --resume
        """
    )
    
    parser.add_argument(
        'base_dir',
        help='Artifactory filestoreåŸºç¡€ç›®å½•è·¯å¾„'
    )
    parser.add_argument(
        '--threads', '-t',
        type=int,
        default=4,
        help='å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤: 4)'
    )
    parser.add_argument(
        '--batch-size', '-b',
        type=int,
        default=10000,
        help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 10000)'
    )
    parser.add_argument(
        '--start-time',
        help='å¼€å§‹æ—¶é—´ (æ ¼å¼: YYYY-MM-DD HH:MM)'
    )
    parser.add_argument(
        '--end-time',
        help='ç»“æŸæ—¶é—´ (æ ¼å¼: YYYY-MM-DD HH:MM)'
    )
    parser.add_argument(
        '--resume', '-r',
        action='store_true',
        help='ä»ä¸Šæ¬¡è¿›åº¦ç»§ç»­'
    )
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='è¯¦ç»†è¾“å‡º'
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.base_dir):
        logger.error(f"ç›®å½•ä¸å­˜åœ¨: {args.base_dir}")
        sys.exit(1)
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = SHA1Validator(args.base_dir, args.threads, args.batch_size)
    
    # è‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½è¿›åº¦ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    if validator.load_progress():
        logger.info("æ£€æµ‹åˆ°ä¸Šæ¬¡è¿›åº¦ï¼Œè‡ªåŠ¨ç»§ç»­...")
    else:
        logger.info("å¼€å§‹æ–°çš„éªŒè¯ä»»åŠ¡...")
    
    try:
        # æµå¼å¤„ç†
        validator.validate_files_streaming(args.start_time, args.end_time)
        
        # ä¿å­˜æœ€ç»ˆè¿›åº¦
        validator.save_progress()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = validator.generate_report()
        print(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        with open('validation_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info("éªŒè¯å®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜åˆ° validation_report.txt")
        
        # è¿”å›é€‚å½“çš„é€€å‡ºç 
        if validator.results['invalid'] > 0 or validator.results['errors'] > 0:
            sys.exit(1)
        else:
            sys.exit(0)
            
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜è¿›åº¦...")
        # ä¸­æ–­æ—¶ä¿å­˜å½“å‰æ‰¹æ¬¡è®¡æ•°
        validator.save_progress()
        sys.exit(130)
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 